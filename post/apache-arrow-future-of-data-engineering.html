<!DOCTYPE html>
<html lang="en" class="relative h-full antialiased dark">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="icon" href="/favicon.svg" />
    <meta http-equiv="content-security-policy" content="" />
    <link href="../_app/immutable/assets/_layout-55b2e92d.css" rel="stylesheet" />
    <link href="../_app/immutable/assets/_page-3805f4c7.css" rel="stylesheet" />
    <link rel="modulepreload" href="../_app/immutable/start-84fdaa70.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/index-0daa1a08.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/singletons-d58f7749.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/preload-helper-41c905a7.js" />
    <link
      rel="modulepreload"
      href="../_app/immutable/components/pages/_layout.svelte-68c92546.js"
    />
    <link rel="modulepreload" href="../_app/immutable/modules/pages/_layout.js-9cbb603b.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/_layout-da46b06b.js" />
    <link
      rel="modulepreload"
      href="../_app/immutable/components/pages/post/_slug_/_page.svelte-457618df.js"
    />
    <link rel="modulepreload" href="../_app/immutable/chunks/info-523b83f0.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/PostDate-20ade620.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/ArrowLeftIcon-0d013bae.js" />
    <link rel="modulepreload" href="../_app/immutable/chunks/SocialLinks-1a8fffbb.js" />
    <link
      rel="modulepreload"
      href="../_app/immutable/modules/pages/post/_slug_/_page.js-99cca405.js"
    />
    <link rel="modulepreload" href="../_app/immutable/chunks/_page-d05ff059.js" />
    <title>
      How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem - Rafael "Auyer" Passos
    </title>
    <!-- HEAD_svelte-7wmra1_START -->
    <meta
      name="description"
      content="In the field of Data Engineering, the Apache Spark framework is one of the most known and powerful ways to extract and process data.
It is well-trusted, and it is also very simple to use once you get the infrastructure set up.
Understandably, most engineers will choose it for every task.
However, in a lot of ways, it can be overkill. And a very expensive one."
    />
    <meta name="author" content='Rafael "Auyer" Passos' />
    <meta property="og:url" content="https://rcpassos.me/apache-arrow-future-of-data-engineering" />
    <meta property="og:type" content="website" />
    <meta
      property="og:title"
      content="How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem"
    />
    <meta
      property="og:description"
      content="In the field of Data Engineering, the Apache Spark framework is one of the most known and powerful ways to extract and process data.
It is well-trusted, and it is also very simple to use once you get the infrastructure set up.
Understandably, most engineers will choose it for every task.
However, in a lot of ways, it can be overkill. And a very expensive one."
    />
    <meta
      property="og:image"
      content="https://og-image.vercel.app/**How%20I%20Decreased%20ETL%20Cost%20by%20Leveraging%20the%20Apache%20Arrow%20Ecosystem**?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fhyper-color-logo.svg"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <meta property="twitter:domain" content="https://rcpassos.me" />
    <meta
      property="twitter:url"
      content="https://rcpassos.me/apache-arrow-future-of-data-engineering"
    />
    <meta
      name="twitter:title"
      content="How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem"
    />
    <meta
      name="twitter:description"
      content="In the field of Data Engineering, the Apache Spark framework is one of the most known and powerful ways to extract and process data.
It is well-trusted, and it is also very simple to use once you get the infrastructure set up.
Understandably, most engineers will choose it for every task.
However, in a lot of ways, it can be overkill. And a very expensive one."
    />
    <meta
      name="twitter:image"
      content="https://og-image.vercel.app/**How%20I%20Decreased%20ETL%20Cost%20by%20Leveraging%20the%20Apache%20Arrow%20Ecosystem**?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fhyper-color-logo.svg"
    />
    <!-- HEAD_svelte-7wmra1_END -->
    <script>
      let darkModeMediaQuery = window.matchMedia('(prefers-color-scheme: dark)')

      updateMode()
      darkModeMediaQuery.addEventListener('change', updateModeWithoutTransitions)
      window.addEventListener('storage', updateModeWithoutTransitions)

      function updateMode() {
        let isSystemDarkMode = darkModeMediaQuery.matches
        let isDarkMode =
          window.localStorage.isDarkMode === 'true' ||
          (!('isDarkMode' in window.localStorage) && isSystemDarkMode)

        if (isDarkMode) {
          document.documentElement.classList.add('dark')
        } else {
          document.documentElement.classList.remove('dark')
        }

        if (isDarkMode === isSystemDarkMode) {
          delete window.localStorage.isDarkMode
        }
      }

      function disableTransitionsTemporarily() {
        document.documentElement.classList.add('[&_*]:!transition-none')
        window.setTimeout(() => {
          document.documentElement.classList.remove('[&_*]:!transition-none')
        }, 0)
      }

      function updateModeWithoutTransitions() {
        disableTransitionsTemporarily()
        updateMode()
      }
    </script>
  </head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4L7GTF77EH"></script>
  <script>
    window.dataLayer = window.dataLayer || []
    function gtag() {
      dataLayer.push(arguments)
    }
    gtag('js', new Date())

    gtag('config', 'G-4L7GTF77EH')
  </script>

  <body class="h-full bg-white dark:bg-zinc-900 text-zinc-600 dark:text-zinc-400">
    <div>
      <div class="flex flex-col min-h-screen">
        <div class="flex flex-col flex-grow w-full px-4 py-2">
          <header class="flex items-center justify-between w-full max-w-4xl py-4 mx-auto lg:pb-8">
            <nav class="nav flex items-center w-full mx-auto svelte-1r9m0vj">
              <a class="logolink" href="/" active="true"
                ><img src="/assets/logo.svg" alt="Auyer" width="30px" height="30px"
              /></a>
              <a class="link svelte-1r9m0vj" href="/about" active="false">About </a>
              <a class="link svelte-1r9m0vj" href="/posts" active="false">Posts </a>
            </nav>
            <button
              type="button"
              role="switch"
              aria-label="Toggle Dark Mode"
              aria-checked="true"
              class="w-5 h-5 sm:h-8 sm:w-8 sm:p-1"
            >
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 20 20"
                fill="currentColor"
                class="hidden text-zinc-500 dark:block"
              >
                <path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path>
              </svg>
              <svg
                xmlns="http://www.w3.org/2000/svg"
                viewBox="0 0 20 20"
                fill="currentColor"
                class="block text-zinc-400 dark:hidden"
              >
                <path
                  fill-rule="evenodd"
                  d="M10 2a1 1 0 011 1v1a1 1 0 11-2 0V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95l.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707a1 1 0 11-1.414-1.414l.707-.707a1 1 0 011.414 0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707a1 1 0 00-1.414 1.414l.707.707zm1.414 8.486l-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z"
                  clip-rule="evenodd"
                ></path>
              </svg>
            </button>
          </header>
          <main>
            <div class="root max-w-2xl mx-auto lg:max-w-none svelte-1eui11k">
              <div class="hidden lg:block pt-8">
                <div class="sticky top-0 w-full flex justify-end pt-11 pr-8">
                  <a
                    class="items-center justify-center hidden w-10 h-10 mb-8 transition bg-white rounded-full shadow-md -top-1 -left-16 lg:flex group shadow-zinc-800/5 ring-1 ring-zinc-900/5 dark:border dark:border-zinc-700/50 dark:bg-zinc-800 dark:ring-0 dark:focus-visible:ring-2 dark:ring-white/10 dark:hover:border-zinc-700 dark:hover:ring-white/20"
                    href="/posts"
                    aria-label="Go back to posts"
                    ><svg
                      xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 20 20"
                      fill="currentColor"
                      class="w-4 h-4 transition stroke-zinc-500 group-hover:stroke-zinc-700 dark:stroke-zinc-500 dark:group-hover:stroke-zinc-400"
                    >
                      <path
                        fill-rule="evenodd"
                        d="M9.707 14.707a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414l4-4a1 1 0 011.414 1.414L7.414 9H15a1 1 0 110 2H7.414l2.293 2.293a1 1 0 010 1.414z"
                        clip-rule="evenodd"
                      ></path></svg
                  ></a>
                </div>
              </div>

              <div class="w-full mx-auto overflow-x-hidden">
                <article>
                  <header class="flex flex-col">
                    <h1
                      class="mt-6 text-4xl font-bold tracking-tight text-zinc-800 dark:text-zinc-100 sm:text-5xl"
                    >
                      How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem
                    </h1>
                    <div
                      class="relative z-10 order-first mb-3 flex text-zinc-500 dark:text-zinc-400 text-sm sm:text-base pl-3.5"
                    >
                      <span
                        class="absolute inset-y-0 left-0 flex items-center py-1"
                        aria-hidden="true"
                        ><span class="h-full w-0.5 rounded-full bg-zinc-200 dark:bg-zinc-500"></span
                      ></span>
                      <div class="flex">
                        <time datetime="2023-02-01">February 1, 2023</time>
                        <span class="mx-1">â€¢</span>
                        <span>7 min read</span>
                      </div>
                    </div>
                  </header>

                  <div class="prose dark:prose-invert">
                    <blockquote>
                      <p>
                        In the field of Data Engineering, the Apache Spark framework is one of the
                        most known and powerful ways to extract and process data. It is
                        well-trusted, and it is also very simple to use once you get the
                        infrastructure set up. Understandably, most engineers will choose it for
                        every task. However, in a lot of ways, it can be overkill. And a very
                        expensive one.
                      </p>
                    </blockquote>
                    <h2 id="our-data-platform-journey">
                      <a href="#our-data-platform-journey">Our data platform journey</a>
                    </h2>
                    <p>
                      In our stack, we manage several databases for different microservices that our
                      team built. We also have a few legacy databases for two platforms built by
                      external companies.
                    </p>
                    <p>
                      To bring insights and enable data-driven decision-making for our directors, we
                      needed a way to run analytical queries on all of them. Enabling it requires we
                      run ETLs (extract, transform, and load scripts) to get data from the databases
                      into our Data Lake.
                    </p>
                    <p>
                      At the time of writing this article, I am the professional responsible for
                      most of the data integrations and extractions at my company. And I was also
                      the lead architect for our data platform and infrastructure, and I will
                      explain how it evolved into what we have today.
                    </p>
                    <h3 id="extraction-jobs-what-an-etl-looks-like">
                      <a href="#extraction-jobs-what-an-etl-looks-like"
                        >Extraction Jobs: what an ETL looks like</a
                      >
                    </h3>
                    <p>
                      <img
                        src="/_app/immutable/assets/ETL_diagram-8c98227f.svg"
                        alt="ETL (Extract Transform Load) diagram"
                      />
                      ETL (Extract Transform Load) diagram
                    </p>
                    <p>
                      The first iteration of our ETL system used the AWS Glue Jobs service. It was
                      easy to get started with since it is a serverless offering of Apache Spark
                      (with some customizations on top).
                    </p>
                    <p>
                      These extractions would take all data from the databases and save all of it as
                      Apache Parquet files in an S3 bucket, separating databases into folders, and
                      their tables as subfolders. We use AWS Athena (similar to the Open Source
                      Presto/Trino project) to run SQL queries on all these files as if it was one
                      big Data-Warehouse.
                    </p>
                    <p>
                      At the end of the stack, we have Metabase as our visualization tool, for
                      making beautiful dashboards. These last two are still part of our stack. Weâ€™ve
                      tested different projects, like Dremio and Trino for running the queries, and
                      also Apache Superset for the viz, but ended up sticking with our first
                      choices. What I changed the most is how we run the extraction scripts.
                    </p>
                    <p>
                      The first change was to migrate a few ETLs to AWS EMR (Managed Apache Spark
                      clusters). These scripts had become too complex to run on Glue, and on EMR we
                      can scale the cluster as we wish. The cost of running them was also a good
                      reason to migrate since AWS Glue can be a lot more expensive than the
                      alternatives.
                    </p>
                    <p>
                      The second step was to relinquish Amazonâ€™s help in managing these clusters and
                      do it myself on Kubernetes. I used Spark
                      <a
                        href="https://github.com/GoogleCloudPlatform/spark-on-k8s-operator"
                        rel="nofollow"
                        >spark-on-k8s-operator</a
                      >
                      to run our Spark Jobs. It made them a lot faster (start-up times for EMR are
                      painfully slow), cheaper, and easier to manage. With this operator, jobs are
                      submitted as Kubernetes custom resources, and the operator creates all
                      necessary pods to run the scripts.
                    </p>
                    <p>
                      Up to this point, weâ€™ve been writing Apache Spark scripts and only changing
                      how and where we run them. The last step is different. Using Apache Arrow and
                      just simple containers, I made most of the old extractions obsolete.
                    </p>
                    <h2 id="apache-spark-vs-apache-arrow-not-equivalent">
                      <a href="#apache-spark-vs-apache-arrow-not-equivalent"
                        >Apache Spark vs Apache Arrow (not equivalent)</a
                      >
                    </h2>
                    <p>
                      Apache Spark is made for distributed work. For this, it is usually set up in a
                      cluster with one or a few driver/master nodes, and at least a few executor
                      nodes. This is amazing when you need to work with large datasets, and they
                      canâ€™t fit into the memory of one reasonably priced machine. But there is
                      always a downside. Even when this is the case, distributing the processing
                      will not always be perfect. It could be necessary to share data between nodes,
                      and this causes a lot of network traffic. And some operations just need data
                      to be in memory.
                    </p>
                    <p>
                      In the other case, when the workload is not that large, distributing it will
                      yield no real gain. It will most likely hurt it due to various types of
                      overhead like synchronization and transport. You can run Apache Spark without
                      a cluster, but it was not made for that. For this reason, I decided to test
                      some new projects that would do what I needed without being distributed.
                    </p>
                    <p>
                      The tools I used to create our new extractions are
                      <a href="https://www.pola.rs/" rel="nofollow">Polars</a> and
                      <a href="https://github.com/sfu-db/connector-x" rel="nofollow">ConnectorX</a>.
                      Polars is a dataframe library, like pandas, but implemented in Rust with the
                      Apache Arrow data model. It is a columnar data model, and it was created to be
                      implemented in any language. But thatâ€™s not the impressive part: you can share
                      data between totally different codebases, like Rust, Java, and Python, without
                      serializing and even reallocating it. If you can access the same space in
                      memory, you can use it with the Arrow ecosystem. When I said Spark shuffles
                      cause a lot of traffic over the network, it also requires all this data to be
                      serialized before sending and deserialized at the receiving end. This wastes a
                      lot of time and resources.
                    </p>
                    <h2 id="code-time"><a href="#code-time">Code Time</a></h2>
                    <p>
                      ConnectorX is integrated into Polars, and if both are installed, you can call
                      <code>polars.read_sql</code>. I will use it direclty though:
                    </p>
                    <pre
                      class="language-undefined"
                    ><!-- HTML_TAG_START --><code class="language-undefined">import connectorx as cx

arrow_table = cx.read_sql(
    query=&quot;select * from table_x&quot;,
    conn=&quot;postgres://user:pass@host:port/database&quot;,
    return_type=&quot;arrow2&quot;,
    protocol=&quot;binary&quot;,
)
</code><!-- HTML_TAG_END --></pre>
                    <p>
                      My example uses Postgres, but ConnectorX supports several other databases. The
                      output of this function is an Apache Arrow Table. Arrow2 means it uses the
                      unofficial Rust implementation
                      <a href="https://github.com/jorgecarleitao/arrow2" rel="nofollow">Arrow2</a>.
                      There is also the official Apache Arrow implementation in Rust, and a C++
                      implementation used by PyArrow.
                    </p>
                    <p>
                      This next call instructs Polars to read the arrow table memory space. And as
                      the Docs say:
                    </p>
                    <blockquote>
                      <p>
                        This operation will be zero copy for the most part. Types that are not
                        supported by Polars may be cast to the closest supported type.
                      </p>
                    </blockquote>
                    <pre
                      class="language-undefined"
                    ><!-- HTML_TAG_START --><code class="language-undefined">import polars as pl
df = pl.from_arrow(arrow_table)</code><!-- HTML_TAG_END --></pre>
                    <p>
                      Once you loaded it into Polars, you can manipulate this data at will. There is
                      an option to turn it into a lazy operation like it is done with Apache Spark.
                      This is very useful because it allows Polars to optimize the query plan when
                      possible. This also enables the use of streaming, for larger-than-memory
                      operations.
                    </p>
                    <p>
                      In this sample, I will keep it simple and write it to a file directly. This
                      could be done to a local path, or almost any destination
                      <a href="https://filesystem-spec.readthedocs.io/en/latest/" rel="nofollow"
                        >fsspec</a
                      >
                      supports. For example, one valid path could be
                      <code>s3://bucket/database/folder/</code>. If you do not specify the file
                      name, it will generate a random one. If you want to keep a single file, or
                      want to replace an existing one, make sure to specify the file name.
                    </p>
                    <pre
                      class="language-undefined"
                    ><!-- HTML_TAG_START --><code class="language-undefined">df.write_parquet(&quot;output.snappy.parquet&quot;, compression:&quot;snappy&quot;)</code><!-- HTML_TAG_END --></pre>
                    <p>
                      It is also possible to use PyArrow to do this. As I said before, PyArrow uses
                      the C++ implementation of Arrow. But data can flow between them seamlessly
                      without the need for serialization, or even memory copying.
                    </p>
                    <pre
                      class="language-undefined"
                    ><!-- HTML_TAG_START --><code class="language-undefined">import pyarrow.parquet as pq

pq.write_table(
    df.to_arrow(),
    where=&quot;output.snappy.parquet&quot;,
    compression=&quot;snappy&quot;,
)</code><!-- HTML_TAG_END --></pre>
                    <p>
                      I created an example repository in GitHub that puts all of this together.
                      Check it out!
                    </p>
                    <p>
                      <a href="https://github.com/auyer/polars-extraction" rel="nofollow"
                        >github.com/auyer/polars-extraction</a
                      >
                    </p>
                    <h2 id="conclusion"><a href="#conclusion">Conclusion</a></h2>
                    <p>
                      Apache Spark is an established framework for building complex ETLs. But it
                      carries a heavy JVM stack behind it. As we discussed here, it is not a good
                      choice for small datasets if you are worried about cost (you should be).
                    </p>
                    <p>
                      The Apache Arrow ecosystem is growing. It canâ€™t replace Spark just yet, but
                      one day I bet it will. But when doing what it is able to do now, it does it a
                      lot faster and consumes fewer machine resources. With new features being
                      implemented into Polars almost every week, it will soon be the ubiquitous tool
                      for data frames.
                    </p>
                    <p>
                      ConnectorX is an important piece for this success. It does not have all the
                      features it needs to make Polars fully replace Spark for me, as it does not
                      support all Postgres Types. I implemented support for a few, like Enum and
                      ltree, but others are still missing, like string arrays. It could receive more
                      love from the community.
                    </p>
                    <p>Hope this article was worth reading! Thanks!</p>
                  </div>
                </article>

                <hr />
                <div class="py-8">
                  <div class="grid gap-8">
                    <div class="flex justify-center order-1 col-span-2 gap-6 md:order-2">
                      <a href="https://github.com/auyer" class="group" aria-label="Follow on GitHub"
                        ><svg
                          viewBox="0 0 24 24"
                          aria-hidden="true"
                          class="w-6 h-6 transition fill-zinc-500 group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"
                        >
                          <path
                            fillrule="evenodd"
                            cliprule="evenodd"
                            d="M12 2C6.475 2 2 6.588 2 12.253c0 4.537 2.862 8.369 6.838 9.727.5.09.687-.218.687-.487 0-.243-.013-1.05-.013-1.91C7 20.059 6.35 18.957 6.15 18.38c-.113-.295-.6-1.205-1.025-1.448-.35-.192-.85-.667-.013-.68.788-.012 1.35.744 1.538 1.051.9 1.551 2.338 1.116 2.912.846.088-.666.35-1.115.638-1.371-2.225-.256-4.55-1.14-4.55-5.062 0-1.115.387-2.038 1.025-2.756-.1-.256-.45-1.307.1-2.717 0 0 .837-.269 2.75 1.051.8-.23 1.65-.346 2.5-.346.85 0 1.7.115 2.5.346 1.912-1.333 2.75-1.05 2.75-1.05.55 1.409.2 2.46.1 2.716.637.718 1.025 1.628 1.025 2.756 0 3.934-2.337 4.806-4.562 5.062.362.32.675.936.675 1.897 0 1.371-.013 2.473-.013 2.82 0 .268.188.589.688.486a10.039 10.039 0 0 0 4.932-3.74A10.447 10.447 0 0 0 22 12.253C22 6.588 17.525 2 12 2Z"
                          ></path></svg
                      ></a>

                      <a
                        href="https://www.linkedin.com/in/passosRafael"
                        class="group"
                        aria-label="Follow on LinkedIn"
                        ><svg
                          viewBox="0 0 24 24"
                          class="w-6 h-6 transition fill-zinc-500 group-hover:fill-zinc-600 dark:fill-zinc-400 dark:group-hover:fill-zinc-300"
                        >
                          <path
                            d="M18.335 18.339H15.67v-4.177c0-.996-.02-2.278-1.39-2.278-1.389 0-1.601 1.084-1.601 2.205v4.25h-2.666V9.75h2.56v1.17h.035c.358-.674 1.228-1.387 2.528-1.387 2.7 0 3.2 1.778 3.2 4.091v4.715zM7.003 8.575a1.546 1.546 0 01-1.548-1.549 1.548 1.548 0 111.547 1.549zm1.336 9.764H5.666V9.75H8.34v8.589zM19.67 3H4.329C3.593 3 3 3.58 3 4.297v15.406C3 20.42 3.594 21 4.328 21h15.338C20.4 21 21 20.42 21 19.703V4.297C21 3.58 20.4 3 19.666 3h.003z"
                          ></path></svg
                      ></a>

                      <a
                        href="https://stackoverflow.com/users/5621569/auyer?tab=profile"
                        class="group"
                        aria-label="Check me on StackOverflow"
                        ><svg
                          class="svelte-fa svelte-1cj2gr0"
                          style="
                            height: 1em;
                            vertical-align: -0.125em;
                            transform-origin: center;
                            overflow: visible;
                          "
                          viewBox="0 0 384 512"
                          aria-hidden="true"
                          role="img"
                          xmlns="http://www.w3.org/2000/svg"
                        >
                          <g transform="translate(192 256)" transform-origin="96 0">
                            <g transform="translate(0,0) scale(1,1)">
                              <path
                                d="M290.7 311L95 269.7 86.8 309l195.7 41zm51-87L188.2 95.7l-25.5 30.8 153.5 128.3zm-31.2 39.7L129.2 179l-16.7 36.5L293.7 300zM262 32l-32 24 119.3 160.3 32-24zm20.5 328h-200v39.7h200zm39.7 80H42.7V320h-40v160h359.5V320h-40z"
                                fill="currentColor"
                                transform="translate(-192 -256)"
                              ></path>
                            </g>
                          </g></svg
                      ></a>

                      <a
                        href="https://medium.com/@rcpassos"
                        class="group"
                        aria-label="Check me on Medium"
                        ><svg
                          class="svelte-fa svelte-1cj2gr0"
                          style="
                            height: 1em;
                            vertical-align: -0.125em;
                            transform-origin: center;
                            overflow: visible;
                          "
                          viewBox="0 0 640 512"
                          aria-hidden="true"
                          role="img"
                          xmlns="http://www.w3.org/2000/svg"
                        >
                          <g transform="translate(320 256)" transform-origin="160 0">
                            <g transform="translate(0,0) scale(1,1)">
                              <path
                                d="M180.5 74.26C80.81 74.26 0 155.6 0 256S80.82 437.7 180.5 437.7 361 356.4 361 256 280.2 74.26 180.5 74.26zm288.3 10.65c-49.85 0-90.25 76.62-90.25 171.1s40.41 171.1 90.25 171.1 90.25-76.62 90.25-171.1H559C559 161.5 518.6 84.91 468.8 84.91zm139.5 17.82c-17.53 0-31.74 68.63-31.74 153.3s14.2 153.3 31.74 153.3S640 340.6 640 256C640 171.4 625.8 102.7 608.3 102.7z"
                                fill="currentColor"
                                transform="translate(-320 -256)"
                              ></path>
                            </g>
                          </g></svg
                      ></a>
                    </div>
                    <div class="flex justify-center order-2 md:order-1 md:col-span-2">
                      <a href="/" class="inline-block rounded-full"
                        ><img
                          src="/assets/profile.png"
                          alt='Rafael "Auyer" Passos'
                          class="w-24 h-24 mx-auto rounded-full md:w-28 md:h-28 ring-2 ring-zinc-200 dark:ring-zinc-700"
                      /></a>
                    </div>
                    <p class="order-3 text-base text-zinc-600 dark:text-zinc-400">
                      I'm Rafael, a software engineer based in Brazil. I began my career as a
                      researcher, but migrated to the tech industry in 2019. I'm a curious
                      individual. I started with (web) software development in Golang. Due to
                      company needs and my interests, I specialized in Data Engineering and even led
                      a squad. Besides all that, I worked with a fair share of Cloud Infrastructure
                      and Kubernetes, developed in Python and Rust, and other things.
                    </p>
                  </div>
                </div>
              </div>

              <div class="hidden xl:block pt-10">
                <aside
                  class="sticky hidden w-48 ml-8 xl:block top-8"
                  aria-label="Table of Contents"
                >
                  <div class="relative flex flex-col items-start group">
                    <div class="relative z-10 flex-1 text-sm text-zinc-600 dark:text-zinc-400">
                      <ul class="flex flex-col gap-2">
                        <li
                          class="pl-2 transition-colors border-teal-500 heading text-zinc-500 dark:text-zinc-600 hover:text-zinc-900 dark:hover:text-zinc-100 svelte-t48kcq active"
                          style="--depth: 1"
                        >
                          <a href="#our-data-platform-journey">Our data platform journey</a>
                        </li>
                        <li
                          class="pl-2 transition-colors border-teal-500 heading text-zinc-500 dark:text-zinc-600 hover:text-zinc-900 dark:hover:text-zinc-100 svelte-t48kcq"
                          style="--depth: 2"
                        >
                          <a href="#extraction-jobs-what-an-etl-looks-like"
                            >Extraction Jobs: what an ETL looks like</a
                          >
                        </li>
                        <li
                          class="pl-2 transition-colors border-teal-500 heading text-zinc-500 dark:text-zinc-600 hover:text-zinc-900 dark:hover:text-zinc-100 svelte-t48kcq"
                          style="--depth: 1"
                        >
                          <a href="#apache-spark-vs-apache-arrow-not-equivalent"
                            >Apache Spark vs Apache Arrow (not equivalent)</a
                          >
                        </li>
                        <li
                          class="pl-2 transition-colors border-teal-500 heading text-zinc-500 dark:text-zinc-600 hover:text-zinc-900 dark:hover:text-zinc-100 svelte-t48kcq"
                          style="--depth: 1"
                        >
                          <a href="#code-time">Code Time</a>
                        </li>
                        <li
                          class="pl-2 transition-colors border-teal-500 heading text-zinc-500 dark:text-zinc-600 hover:text-zinc-900 dark:hover:text-zinc-100 svelte-t48kcq"
                          style="--depth: 1"
                        >
                          <a href="#conclusion">Conclusion</a>
                        </li>
                      </ul>
                    </div>
                  </div>
                </aside>
              </div>
            </div>
            <footer class="footer svelte-uoqvvb">
              Â© 2023 Rafael Passos
              <small class="line">ðŸš€ Built with SvelteKit</small>
            </footer>
          </main>
        </div>
      </div>

      <script type="module" data-sveltekit-hydrate="j9j3n7">
        import { start } from '../_app/immutable/start-84fdaa70.js'

        start({
          assets: '',
          env: {},
          target: document.querySelector('[data-sveltekit-hydrate="j9j3n7"]').parentNode,
          version: '1676424337922',
          hydrate: {
            node_ids: [0, 6],
            data: [
              null,
              {
                type: 'data',
                data: {
                  post: {
                    title: 'How I Decreased ETL Cost by Leveraging the Apache Arrow Ecosystem',
                    date: '2023-02-01',
                    headings: [
                      {
                        depth: 2,
                        value: 'Our data platform journey',
                        id: 'our-data-platform-journey'
                      },
                      {
                        depth: 3,
                        value: 'Extraction Jobs: what an ETL looks like',
                        id: 'extraction-jobs-what-an-etl-looks-like'
                      },
                      {
                        depth: 2,
                        value: 'Apache Spark vs Apache Arrow (not equivalent)',
                        id: 'apache-spark-vs-apache-arrow-not-equivalent'
                      },
                      { depth: 2, value: 'Code Time', id: 'code-time' },
                      { depth: 2, value: 'Conclusion', id: 'conclusion' }
                    ],
                    slug: 'apache-arrow-future-of-data-engineering',
                    isIndexFile: true,
                    preview: {
                      html: '\u003Cp\u003EIn the field of Data Engineering, the Apache Spark framework is one of the most known and powerful ways to extract and process data.\nIt is well-trusted, and it is also very simple to use once you get the infrastructure set up.\nUnderstandably, most engineers will choose it for every task.\nHowever, in a lot of ways, it can be overkill. And a very expensive one.\u003C\u002Fp\u003E',
                      text: 'In the field of Data Engineering, the Apache Spark framework is one of the most known and powerful ways to extract and process data.\nIt is well-trusted, and it is also very simple to use once you get the infrastructure set up.\nUnderstandably, most engineers will choose it for every task.\nHowever, in a lot of ways, it can be overkill. And a very expensive one.'
                    },
                    readingTime: '7 min read',
                    next: void 0,
                    previous: {
                      title: 'Going Fast With Go - an introduction to Golang (slides)',
                      date: '2019-08-01',
                      headings: [],
                      slug: 'going-fast-with-go',
                      isIndexFile: true,
                      preview: {
                        html: '\u003Cp\u003EThis is a (small) presentation I gave to my coworkers at CEPESC. At the time, most of the software there was written in Java, and our team (researchers from the Federal University of Brasilia) was developing in Python. I was advocating using Golang in some performance-critical areas of our system.\u003C\u002Fp\u003E',
                        text: 'This is a (small) presentation I gave to my coworkers at CEPESC. At the time, most of the software there was written in Java, and our team (researchers from the Federal University of Brasilia) was developing in Python. I was advocating using Golang in some performance-critical areas of our system.'
                      },
                      readingTime: '1 min read'
                    }
                  }
                },
                uses: { params: ['slug'] }
              }
            ],
            form: null,
            error: null
          }
        })
      </script>
    </div>
  </body>
</html>
